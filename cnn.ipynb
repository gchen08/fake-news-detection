{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Input, concatenate\n",
    "from keras.layers.core import Flatten, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "NUM_WORDS = 2000\n",
    "EMBEDDING_DIM = 64\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 291430\n",
      "Train and val X tensor: (40986, 150) (10247, 150)\n",
      "Train and val label tensor: (40986, 2) (10247, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data\\\\all_news_tokenized.csv\", encoding=\"utf-8\").replace((True, False), (\"true_news\", \"fake_news\"))\n",
    "\n",
    "# 划分训练集与测试集\n",
    "ratio = .7\n",
    "thr = int(len(df) * ratio)\n",
    "train_data = df[: thr]\n",
    "test_data = df[thr:]\n",
    "\n",
    "# 标签数字化\n",
    "creds = train_data.label.unique()\n",
    "dic = {}\n",
    "for i, cred in enumerate(creds):\n",
    "    dic[cred] = i\n",
    "labels = train_data.label.apply(lambda x: dic[x])\n",
    "\n",
    "# 抽样构建验证集\n",
    "val_data = train_data.sample(frac=0.2, random_state=200)\n",
    "train_data = train_data.drop(val_data.index)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_data.content)\n",
    "sequence_train = tokenizer.texts_to_sequences(train_data.content)\n",
    "sequence_valid = tokenizer.texts_to_sequences(val_data.content)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Unique tokens: %s\" % len(word_index))\n",
    "\n",
    "pad_len = min(MAX_LEN, len(sequence_train))\n",
    "X_train = sequence.pad_sequences(sequence_train, maxlen=pad_len)\n",
    "X_val = sequence.pad_sequences(sequence_valid, maxlen=pad_len)\n",
    "y_train = to_categorical(np.asanyarray(labels[train_data.index]))\n",
    "y_val = to_categorical(np.asanyarray(labels[val_data.index]))\n",
    "print(\"Train and val X tensor:\", X_train.shape, X_val.shape)\n",
    "print(\"Train and val label tensor:\", y_train.shape, y_val.shape)\n",
    "\n",
    "# embedding\n",
    "vocabulary_size = min(len(word_index) + 1, NUM_WORDS)\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM)\n",
    "\n",
    "# CNN\n",
    "\n",
    "sequence_length = X_train.shape[1]\n",
    "filter_size = [3, 4, 5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length, EMBEDDING_DIM, 1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(filters=num_filters,\n",
    "                kernel_size=(filter_size[0], EMBEDDING_DIM),\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(filters=num_filters,\n",
    "                kernel_size=(filter_size[1], EMBEDDING_DIM),\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(filters=num_filters,\n",
    "                kernel_size=(filter_size[2], EMBEDDING_DIM),\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_size[0] + 1, 1), strides=(1, 1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_size[1] + 1, 1), strides=(1, 1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_size[2] + 1, 1), strides=(1, 1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "\n",
    "flatten = Flatten()(merged_tensor)\n",
    "\n",
    "dropout = Dropout(drop)(flatten)\n",
    "\n",
    "output = Dense(units=2,\n",
    "               activation=\"softmax\",\n",
    "               kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "adam = Adam(lr=1e-3)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=adam,\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40986 samples, validate on 10247 samples\n",
      "Epoch 1/10\n",
      "40986/40986 [==============================] - 108s 3ms/step - loss: 0.0729 - acc: 0.9939 - val_loss: 0.0761 - val_acc: 0.9886\n",
      "Epoch 2/10\n",
      "40986/40986 [==============================] - 117s 3ms/step - loss: 0.0649 - acc: 0.9945 - val_loss: 0.0761 - val_acc: 0.9882\n",
      "Running time: 224.90183177502948 Seconds\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "import time\n",
    "tos = time.clock()\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          validation_split=0.1,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=callbacks)\n",
    "toe = time.clock()\n",
    "print(\"Running time: %s Seconds\" % (toe - tos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21957/21957 [==============================] - 22s 988us/step\n",
      "Running time: 21.701373585370447 Seconds\n",
      "test_loss: 0.069066, accuracy: 0.990026\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "sequence_test = tokenizer.texts_to_sequences(test_data.content)\n",
    "X_test = sequence.pad_sequences(sequence_test, maxlen=pad_len)\n",
    "labels = test_data.label.apply(lambda x: dic[x])\n",
    "y_test = to_categorical(np.asanyarray(labels[test_data.index]))\n",
    "tos = time.clock()\n",
    "score = model.evaluate(X_test, y_test)\n",
    "toe = time.clock()\n",
    "print(\"Running time: %s Seconds\" % (toe - tos))\n",
    "print(\"test_loss: %f, accuracy: %f\" % (score[0], score[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
